from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, AsyncGenerator, Union
import json
import asyncio
import os
import re
from string import Template
from agent.core.mcp import MCPToolAgent
from pathlib import Path
from mcp.types import (
    EmbeddedResource,
    GetPromptResult,
    ImageContent,
    PromptMessage,
    Role,
    TextContent,
    TextResourceContents
)
import requests
import aiohttp
from functools import lru_cache


class SecurityError(Exception):
    """å®‰å…¨ç›¸å…³å¼‚å¸¸"""
    pass


@dataclass
class DiffContent:
    file_path: str
    content: str

@dataclass
class StandardContent:
    name: str
    content: str
    case: str

@dataclass 
class ReviewStandards:
    name: str
    standards: List[StandardContent]

@dataclass
class ReviewResult:
    file_path: str
    comments: List[Dict[str, str]]

@dataclass
class ReviewDimensions:  
    dimension: str
    knowledge: List[str]
    frameworks: List[str]  

@dataclass
class MergeRequestInfo:
    project_id: str
    project_url: str
    merge_request_iid: int
    commit_sha: str
    diff_refs: dict

class Reviewer(MCPToolAgent):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®è°ƒæ•´æ‰¹æ¬¡å¤§å°
        self.batch_size = int(os.getenv('REVIEW_BATCH_SIZE', 6))
        # æ§åˆ¶æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°ï¼Œé¿å…APIè¿‡è½½
        self.max_concurrent_batches = min(3, max(1, int(os.getenv('REVIEW_MAX_CONCURRENT', 3))))

    async def initialize(self) -> None:
        # 1. è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        await super().initialize()
        # 2. è·å–review session
        self.review_session = self.get_session('code_review')


    async def run(self, group: Optional[str] = None):
        """å¤„ç†ä»£ç å®¡æ ¸æµç¨‹"""
        if not self.review_session:
           raise Exception('revieweråˆå§‹åŒ–å¤±è´¥, è¯·å…ˆå®‰è£…mcpæœåŠ¡: code_review')
        try:
            async for result in self._execute_review_flow(group):
                yield result
            yield "Done"
        except Exception as e:
            print(f"ä»£ç å®¡æ ¸è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")        

    async def ci_run(self, mr_info: MergeRequestInfo, group: Optional[str] = None):
        """ ci ä»£ç å®¡æ ¸æµç¨‹"""
        try:
            async for result in self._execute_review_flow(group, mr_info):
                yield result
            yield "Done"
        except Exception as e:
            error_comment = f"ğŸš¨ ä»£ç å®¡æ ¸è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:\n```\n{str(e)}\n```"
            res = await self.post_review_comment(mr_info, error_comment) 
            yield res   

    def _parse_mcp_response(self, response, expect_content=True) -> tuple[bool, any]:
        """ç»Ÿä¸€è§£æMCPå“åº”çš„å…¬å…±æ–¹æ³•"""
        if response.isError:
            return False, None
        try:
            content = json.loads(self._get_text(response.content[0]))
            is_error = content.get('isError', False)
            if is_error:
                return False, None
            
            if expect_content:
                result = content.get('content', [])
                return True, result
            return True, content
        except Exception as e:
            return False, None

    async def set_group_mode(self, group) -> bool:
        """è®¾ç½®ä»£ç å®¡æ ¸æ¨¡å¼"""
        res = await self.review_session.call_tool('set_group_name', {
            'group_name': group or 'web'
        })
        success, _ = self._parse_mcp_response(res, expect_content=False)
        return success
    
    async def _execute_review_flow(self, group: Optional[str] = None, mr_info: Optional[MergeRequestInfo] = None):
        """ç»Ÿä¸€çš„å®¡æŸ¥æµç¨‹æ ¸å¿ƒé€»è¾‘"""
        # 1. è®¾ç½®å®¡æ ¸æ¨¡å¼
        success = await self.set_group_mode(group)
        if not success:
            yield f"å½“å‰ä¸æ”¯æŒ{group}ç«¯ä»£ç å®¡æ ¸ï¼Œè¯·è”ç³»å¼€å‘è€…å¢åŠ é…ç½®"
            return
            
        # 2. è·å–diffå†…å®¹
        if mr_info:
            diff_contents = await self.get_mr_diff(mr_info)
            project_info = await self.get_mr_project_info(mr_info)
        else:
            diff_contents = await self.get_diff_contents()
            project_info = await self.get_project_info()
            
        if not diff_contents:
            yield "æ²¡æœ‰è·å–åˆ°diffå†…å®¹"
            return
        
        # 3. åˆ†ææ”¹åŠ¨ç‚¹å’Œç¡®å®šå®¡æ ¸ç»´åº¦
        dimensions = await self.analyze_changes(diff_contents, project_info)
        
        # 4. è·å–å®¡æ ¸æ ‡å‡†
        standards = await self.get_review_standards(dimensions)
        
        # 5. standards summary
        summary = await self.get_standards_summary(standards)
        
        # 6. æ‰§è¡Œä»£ç å®¡æ ¸
        comments = ""
        review_results = self.review_code(diff_contents, summary)
        async for result in review_results:
            if mr_info:
                comments += result
            else:
                yield result
        
        # 7. CIæ¨¡å¼çš„åç»­å¤„ç†
        if mr_info:
            # å‘å¸ƒè¯„è®º
            comment_res = await self.post_review_comment(mr_info, comments)
            yield comment_res
            
            # æ€»ç»“å’Œè®°å½•
            metrics, review_summary = await self.summarize_review(comments, mr_info)
            if metrics and review_summary:
                lark_res = await self.post_to_lark(review_summary)
                yield lark_res
                
                record_cr_res = await self.post_to_lark_sheet(metrics, mr_info)
                yield record_cr_res
            else:
                yield "ç”Ÿæˆç¸½çµå¤±æ•—ï¼Œè·³éè¨˜éŒ„åˆ° Lark"

    async def summarize_review(self, comments: str, mr_info: MergeRequestInfo) -> tuple:
        """
        ä¸€æ¬¡æ€§ç²å–çµæ§‹åŒ–æ•¸æ“šå’Œç¸½çµ
        
        Args:
            comments: ä»£ç¢¼å¯©æŸ¥è©•è«–å…§å®¹
            mr_info: Merge Request ä¿¡æ¯
            
        Returns:
            tuple: (metrics_dict, summary_text)
                - metrics_dict: ç”¨æ–¼è¨˜éŒ„åˆ° sheet çš„çµæ§‹åŒ–æ•¸æ“š
                - summary_text: ç”¨æ–¼ç™¼é€åˆ° Lark ç¾¤çµ„çš„ç¸½çµæ–‡æœ¬
        """
        try:
            # ç²å– prompt
            prompt_response = await self.review_session.get_prompt('get_lark_summary_prompt', {
                'comments': comments
            })
            prompt = self._get_text(prompt_response.messages[0].content)
            
            # èª¿ç”¨ LLM ä¸€æ¬¡
            result = ""
            async for chunk in self.config.llm.generate(prompt):
                result += chunk.text
            
            # æª¢æŸ¥çµæœæ˜¯å¦ç‚ºç©º
            if not result or result.strip() == "":
                print("LLM è¿”å›ç©ºçµæœ")
                return None, None
                
            # å˜—è©¦æå– JSON éƒ¨åˆ†
            try:
                # å¦‚æœçµæœåŒ…å« ```json æ¨™è¨˜ï¼Œæå–å…¶ä¸­çš„å…§å®¹
                if "```json" in result:
                    json_str = result.split("```json")[1].split("```")[0].strip()
                else:
                    # å¦å‰‡å˜—è©¦ç›´æ¥è§£ææ•´å€‹çµæœ
                    json_str = result.strip()
                
                # è§£æ JSON
                data = json.loads(json_str)
                
                # æª¢æŸ¥å¿…è¦çš„å­—æ®µ
                if 'metrics' not in data or 'summary' not in data:
                    print("JSON ç¼ºå°‘å¿…è¦å­—æ®µ")
                    print("åŸå§‹è¼¸å‡ºï¼š", result)
                    return None, None
                
                # æå–çµæ§‹åŒ–æ•¸æ“š
                metrics = data['metrics']
                
                # ç”Ÿæˆäººé¡å¯è®€çš„ç¸½çµ
                summary = f"""### åŸºæœ¬ç»Ÿè®¡
- å®¡æŸ¥æ–‡ä»¶æ•°é‡ï¼š{data['summary']['file_count']}
- ä¸¥é‡é—®é¢˜æ•°é‡ï¼š{data['summary']['critical_issues']}
- ä»£ç è´¨é‡è¯„åˆ†ï¼š{data['summary']['score']}

### ä¸¥é‡é—®é¢˜æ¸…å•
{self._format_critical_problems(data['summary']['critical_problems'])}

### æ€»ä½“è¯„ä¼°
1. åˆå¹¶å»ºè®®ï¼š{'âœ… å¯ä»¥åˆå¹¶' if data['summary']['review']['can_merge'] else 'âŒ éœ€è¦ä¿®æ”¹åå†åˆå¹¶'}
2. é˜»æ–­æ€§é—®é¢˜ï¼š{self._format_blocking_issues(data['summary']['review']['blocking_issues'])}
3. æ€»ä½“ç»“è®ºï¼š{data['summary']['review']['conclusion']}

æŸ¥çœ‹è¯¦ç»†CRç»“æœï¼š{mr_info.project_url}/-/merge_requests/{mr_info.merge_request_iid}"""

                return metrics, summary
                
            except json.JSONDecodeError as e:
                print(f"JSON è§£æéŒ¯èª¤ï¼š{str(e)}")
                print("åŸå§‹è¼¸å‡ºï¼š", result)
                return None, None
                
        except Exception as e:
            print(f"ç”Ÿæˆç¸½çµå¤±æ•—: {str(e)}")
            import traceback
            print("éŒ¯èª¤è©³æƒ…ï¼š")
            print(traceback.format_exc())
            return None, None
            
    def _format_critical_problems(self, problems: list) -> str:
        """æ ¼å¼åŒ–åš´é‡å•é¡Œåˆ—è¡¨"""
        if not problems:
            return "æœ¬æ¬¡å®¡æŸ¥æœªå‘ç°ä¸¥é‡é—®é¢˜"
            
        result = []
        for problem in problems:
            result.append(f"""#### {problem['file']}
                - {problem['description']}
                - å½±å“ï¼š{problem['impact']}
                - å»ºè®®ï¼š{problem['suggestion']}""")
                        
        return "\n\n".join(result)
        
    def _format_blocking_issues(self, issues: list) -> str:
        """æ ¼å¼åŒ–é˜»æ–·æ€§å•é¡Œ"""
        if not issues:
            return "ä¸å­˜åœ¨"
        return "ã€".join(issues)

    async def post_to_lark(self, summary: str):
        """å°†å®¡æ ¸ç»“æœå‘å¸ƒåˆ°Lark"""
        lark_res = await self.review_session.call_tool('send_lark_message', {
            "message": summary
        })
        if lark_res.isError:
            return "å‘å¸ƒåˆ°Larkå¤±è´¥"
        try:
            content = json.loads(self._get_text(lark_res.content[0]))
            comment = content.get('content', [])[0]
            return comment.get('text', '')
        except Exception as e:
            return f"å‘å¸ƒåˆ°Larkå¤±è´¥: {e}"

    async def post_to_lark_sheet(self, metrics, mr_info: MergeRequestInfo):
        """å°†å®¡æ ¸ç»“æœå‘å¸ƒåˆ°Lark sheet"""
        record_cr_res = await self.review_session.call_tool('add_row_to_lark_sheet', {
            "metrics": metrics,
            "mr_info": {
                "project_id": mr_info.project_id,
                "project_url": mr_info.project_url,
                "merge_request_iid": mr_info.merge_request_iid,
                "commit_sha": mr_info.commit_sha or "",
                "diff_refs": mr_info.diff_refs or {}
            }
        })
        if record_cr_res.isError:
            return "å‘å¸ƒåˆ°Lark Sheetå¤±è´¥"
        try:
            content = json.loads(self._get_text(record_cr_res.content[0]))
            comment = content.get('content', [])[0]
            return comment.get('text', '')
        except Exception as e:
            return f"å‘å¸ƒåˆ°Lark Sheetå¤±è´¥: {e}"

    async def get_mr_diff(self, mr_info: MergeRequestInfo):
        """è·å–mr diff""" 
        diff_res = await self.review_session.call_tool('get_gitlab_mr_diff', {
            "project_id": f"{mr_info.project_id}",
            "mr_iid": f"{mr_info.merge_request_iid}"
        })
        success, diff = self._parse_mcp_response(diff_res)
        if not success or not diff:
            return []
        try:
            return [DiffContent(**json.loads(content['text'])) for content in diff]
        except Exception:
            return []

    async def get_mr_project_info(self, mr_info: MergeRequestInfo):
        """è·å– mr projectInfo"""    
        project_info_res = await self.review_session.call_tool('get_framework_info_by_gitlab', {
            "project_id": f"{mr_info.project_id}"
        })
        success, project_info_list = self._parse_mcp_response(project_info_res)
        if not success or not project_info_list:
            return {}
        try:
            project_info = project_info_list[0]
            return {
                'frameworks': project_info.get('text', '')
            }
        except Exception:
            return {}

    async def post_review_comment(self, mr_info: MergeRequestInfo, comment: str):
        """å‘å¸ƒè¯„è®ºåˆ° MR"""    
        comment_res = await self.review_session.call_tool('post_mr_comment', {
            "project_id": f"{mr_info.project_id}",
            "mr_iid": f"{mr_info.merge_request_iid}",
            "comment": comment
        })
        if comment_res.isError:
            return "ç•™è¨€å¤±è´¥"
        # è§£æè¯„è®ºç»“æœ
        try:
            comment_content = json.loads(self._get_text(comment_res.content[0]))
            comment = comment_content.get('content', [])[0]
            return comment.get('text', '')
        except Exception as e:
            return f"ç•™è¨€å¤±è´¥: {e}"

    async def get_diff_contents(self) -> List[DiffContent]:
        """ä»MCP Serverè·å–diffå†…å®¹"""
        diff_res = await self.review_session.call_tool('get_current_working_diff')
        success, diff = self._parse_mcp_response(diff_res)
        if not success or not diff:
            return []
        try:
            return [DiffContent(**json.loads(content['text'])) for content in diff]
        except Exception:
            return []

    async def get_project_info(self) -> Dict[str, str]: 
        """è·å–é¡¹ç›®åŸºæœ¬ä¿¡æ¯"""
        project_info_res = await self.review_session.call_tool('get_project_framework_info')
        success, project_info_list = self._parse_mcp_response(project_info_res)
        if not success or not project_info_list:
            return {}
        try:
            project_info = project_info_list[0]
            return {
                'frameworks': project_info.get('text', '')
            }
        except Exception:
            return {}
    
    async def analyze_changes(self, diff_contents: List[DiffContent], project_info: Dict) -> List[ReviewDimensions]:
        """åˆ†ææ”¹åŠ¨å¹¶ç¡®å®šéœ€è¦çš„å®¡æ ¸ç»´åº¦"""
        # diff_contents to str
        diff_contents_str = "\n".join([f"{diff.file_path}:\n{diff.content}" for diff in diff_contents])
        # project_info to str
        project_info_str = "\n".join([f"{key}: {value}" for key, value in project_info.items()])
        
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œåˆ†æ
        response = await self.review_session.get_prompt('get_analyze_changes_prompt',{
            'code': diff_contents_str, 
            'project_info': project_info_str
        })

        prompt = self._get_text(response.messages[0].content)

        dimensions = ""
        async for chunk in self.config.llm.generate(prompt):
            dimensions += chunk.text
        # è§£æè¿”å›çš„ç»´åº¦
        try:
            return json.loads(''.join(dimensions))
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»´åº¦
            return [{"dimension": "code_quality", "knowledge": [], "frameworks": []}]
    
    async def get_review_standards(self, dimensions: List[ReviewDimensions]) -> List[ReviewStandards]:
        """è·å–æ¯ä¸ªç»´åº¦çš„å®¡æ ¸æ ‡å‡†"""
        tasks = [self.review_session.call_tool('get_knowledge_base_chunks', {**dim}) for dim in dimensions]
        standards_responses = await asyncio.gather(*tasks)  # kb çŸ¥è¯†å¬å›
        
        result = []
        for i, standards_response in enumerate(standards_responses):
            success, standards = self._parse_mcp_response(standards_response)
            if not success or not standards:
                continue
            try:
                result.append(ReviewStandards(
                    name=dimensions[i]['dimension'], 
                    standards=[content['text'] for content in standards]
                ))
            except Exception:
                continue
        return result
    
    async def get_standards_summary(self, standards: List[ReviewStandards]) -> str:
        """å­¦ä¹ å¹¶æ€»ç»“å‡ºå®¡æ ¸æ ‡å‡†"""
        standards_formated = [f"{s.name}: {','.join(s.standards)}" for s in standards or []]
        return  ",".join(standards_formated)
    
    @lru_cache(maxsize=1)
    def _load_review_prompt_template(self) -> str:
        """åŠ è½½ä»£ç å®¡æ ¸æç¤ºè¯æ¨¡æ¿ - LRUç¼“å­˜ä¼˜åŒ–"""
        # æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æ¨¡æ¿è·¯å¾„
        template_path = os.getenv('REVIEW_TEMPLATE_PATH')
        if template_path:
            prompt_file_path = Path(template_path)
        else:
            # é»˜è®¤è·¯å¾„ - ç¡®ä¿åœ¨å½“å‰æ¨¡å—ç›®å½•å†…
            prompt_file_path = Path(__file__).parent / 'prompts' / 'common_code_review.xml'
        
        try:
            # ç¡®ä¿æ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»
            if not prompt_file_path.exists():
                raise FileNotFoundError(f"å®¡æ ¸æç¤ºè¯æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_file_path}")
            
            if not prompt_file_path.is_file():
                raise ValueError(f"æŒ‡å®šè·¯å¾„ä¸æ˜¯æ–‡ä»¶: {prompt_file_path}")
            
            with open(prompt_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # åŸºæœ¬å†…å®¹éªŒè¯
            if not content.strip():
                raise ValueError("æ¨¡æ¿æ–‡ä»¶å†…å®¹ä¸ºç©º")
            
            return content
            
        except FileNotFoundError:
            raise FileNotFoundError(f"å®¡æ ¸æç¤ºè¯æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {prompt_file_path}")
        except PermissionError:
            raise PermissionError(f"æ— æƒé™è¯»å–æ¨¡æ¿æ–‡ä»¶: {prompt_file_path}")
        except Exception as e:
            raise Exception(f"åŠ è½½å®¡æ ¸æç¤ºè¯æ¨¡æ¿å¤±è´¥: {str(e)}")

    def _render_review_prompt(self, code: str, standard: str, total_files: int, 
                             batch_info: str = "", is_batch: bool = False) -> str:
        """æ¸²æŸ“å®¡æ ¸æç¤ºè¯æ¨¡æ¿ï¼Œæ”¯æŒæ‰©å±•æ›´å¤šå˜é‡"""
        template_content = self._load_review_prompt_template()
        
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ¨¡æ¿å˜é‡
        template_vars = {
            "code": self._escape_template_value(code),
            "standard": self._escape_template_value(standard),
            "total_files": str(total_files),
            "batch_info": self._escape_template_value(batch_info),
            "is_batch": "true" if is_batch else "false"
        }
        
        try:
            # ä½¿ç”¨æ›´å®‰å…¨çš„æ¨¡æ¿æ›¿æ¢æ–¹å¼
            # å…ˆå°†åŒèŠ±æ‹¬å·æ ¼å¼è½¬æ¢ä¸º${}æ ¼å¼
            converted_template = self._convert_template_format(template_content)
            
            # ä½¿ç”¨string.Templateè¿›è¡Œå®‰å…¨æ›¿æ¢
            template = Template(converted_template)
            rendered_prompt = template.safe_substitute(**template_vars)

            return rendered_prompt
            
        except Exception as e:
            raise Exception(f"æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {str(e)}")
    
    def _convert_template_format(self, template: str) -> str:
        """å°†{{variable}}æ ¼å¼è½¬æ¢ä¸º${variable}æ ¼å¼ï¼Œä½†åªè½¬æ¢inputéƒ¨åˆ†ï¼Œä¿ç•™outputéƒ¨åˆ†çš„Handlebarsè¯­æ³•"""
        # æ‰¾åˆ°inputéƒ¨åˆ†çš„è¾¹ç•Œ
        input_start = template.find('<input>')
        input_end = template.find('</input>')
        
        if input_start == -1 or input_end == -1:
            # æ²¡æœ‰æ‰¾åˆ°inputæ ‡ç­¾ï¼Œè¯´æ˜å¯èƒ½æ˜¯ç®€å•æ¨¡æ¿ï¼Œè½¬æ¢æ‰€æœ‰å˜é‡
            pattern = r'\{\{(\w+)\}\}'
            return re.sub(pattern, r'${\1}', template)
        
        # åˆ†ç¦»inputéƒ¨åˆ†å’Œå…¶ä»–éƒ¨åˆ†
        before_input = template[:input_start]
        input_section = template[input_start:input_end + 8]  # åŒ…å«</input>
        after_input = template[input_end + 8:]
        
        # åªè½¬æ¢inputéƒ¨åˆ†çš„{{variable}}æ ¼å¼
        pattern = r'\{\{(\w+)\}\}'
        converted_input = re.sub(pattern, r'${\1}', input_section)
        
        # é‡æ–°ç»„è£…æ¨¡æ¿ï¼Œä¿æŒoutputéƒ¨åˆ†çš„Handlebarsè¯­æ³•ä¸å˜
        return before_input + converted_input + after_input
    
    def _escape_template_value(self, value: str) -> str:
        """è½¬ä¹‰æ¨¡æ¿å˜é‡å€¼ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé˜²æ­¢æ¨¡æ¿æ³¨å…¥"""
        if not isinstance(value, str):
            return str(value)
        
        # è½¬ä¹‰å¯èƒ½å¯¼è‡´æ¨¡æ¿æ³¨å…¥çš„å­—ç¬¦
        # ä¿æŠ¤${å’Œ}å­—ç¬¦ï¼Œé˜²æ­¢æ„å¤–çš„æ¨¡æ¿å˜é‡æ›¿æ¢
        escaped_value = value.replace('${', '\\${').replace('}', '\\}')
        
        return escaped_value
    
    async def review_code(self, diff_contents: List[DiffContent], summary: str) -> AsyncGenerator[str, None]:
        """æ‰§è¡Œä»£ç å®¡æ ¸ - ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†ç‰ˆæœ¬"""
        try:
            total_files = len(diff_contents)
            
            # å†³å®šå¤„ç†ç­–ç•¥
            if total_files > self.batch_size:
                # å¤§æ‰¹é‡ï¼šåˆ†ç»„å¤„ç†
                async for chunk in self._review_large_changes(diff_contents, summary):
                    yield chunk
            else:
                # å•æ–‡ä»¶å¹¶è¡Œå¤„ç†
                async for chunk in self._review_parallel_batch(diff_contents, summary):
                    yield chunk        
        except Exception as e:
            yield f"ä»£ç å®¡æ ¸è¿‡ç¨‹å‡ºé”™: {str(e)}"

    async def _review_parallel_batch(self, diff_contents: List[DiffContent], summary: str) -> AsyncGenerator[str, None]:
        """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶"""
        try:
            # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
            async def review_single_file(diff: DiffContent):
                diff_content_str = f"// {diff.file_path}:\n\n{diff.content}\n"
                prompt = self._render_review_prompt(
                    code=diff_content_str,
                    standard=summary,
                    total_files=len(diff_contents),
                    batch_info=f"å¹¶è¡Œå®¡æ ¸æ–‡ä»¶ {diff.file_path}",
                    is_batch=False
                )
                
                result = ""
                async for chunk in self.config.llm.generate(prompt):
                    result += chunk.text
                return f"""ã€{diff.file_path}ã€‘å®¡æŸ¥æŠ¥å‘Š \n --- \n{result}\n\n"""
            
            # å¹¶è¡Œæ‰§è¡Œï¼Œä½†é™åˆ¶å¹¶å‘æ•°ä»¥é¿å…è¿‡è½½
            results = []
            for i in range(0, len(diff_contents), 3):  # æ¯æ¬¡å¤„ç†3ä¸ªæ–‡ä»¶
                batch = diff_contents[i:i+3]
                tasks = [review_single_file(diff) for diff in batch]
                batch_results = await asyncio.gather(*tasks)
                results.extend(batch_results)
            
            # è¾“å‡ºç»“æœ
            for result in results:
                yield result
                
        except Exception as e:
            yield f"å¹¶è¡Œå®¡æ ¸å¤±è´¥: {str(e)}"

    async def _review_large_changes(self, diff_contents: List[DiffContent], 
                                summary: str) -> AsyncGenerator[str, None]:
        """å¤„ç†å¤§å‹å˜æ›´çš„å®¡æ ¸ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¹¶è¡Œæ‰¹å¤„ç†"""
        try:
            # åˆ†ç»„å¤„ç†æ–‡ä»¶
            batches = [diff_contents[i:i + self.batch_size] 
                    for i in range(0, len(diff_contents), self.batch_size)]
            
            total_files = len(diff_contents)
            total_batches = len(batches)
            
            # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡çš„è¾…åŠ©å‡½æ•°
            async def process_batch(batch_index: int, batch: List[DiffContent]) -> tuple[int, str]:
                """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
                try:
                    # åˆå¹¶å½“å‰æ‰¹æ¬¡çš„ diff å†…å®¹
                    combined_diff = "\n".join([
                        f"// {diff.file_path}:\n\n{diff.content}\n"
                        for diff in batch
                    ])
                    
                    # å‡†å¤‡æ‰¹æ¬¡ä¿¡æ¯
                    batch_info = f"ç¬¬ {batch_index + 1}/{total_batches} æ‰¹æ¬¡ï¼Œå½“å‰æ‰¹æ¬¡åŒ…å« {len(batch)} ä¸ªæ–‡ä»¶"
                    
                    # ä½¿ç”¨å…¬å…±å‡½æ•°æ¸²æŸ“æç¤ºè¯
                    prompt = self._render_review_prompt(
                        code=combined_diff,
                        standard=summary,
                        total_files=total_files,
                        batch_info=batch_info,
                        is_batch=True
                    )
                        
                    # æ”¶é›†å½“å‰æ‰¹æ¬¡çš„å®¡æ ¸ç»“æœ
                    batch_result = ""
                    async for chunk in self.config.llm.generate(prompt):
                        batch_result += chunk.text
                    
                    return batch_index, batch_result
                    
                except Exception as e:
                    return batch_index, f"æ‰¹æ¬¡ {batch_index + 1} å¤„ç†å¤±è´¥: {str(e)}"
            
            # æ§åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡è½½
            max_concurrent_batches = min(self.max_concurrent_batches, total_batches)
            all_results = [None] * total_batches  # é¢„åˆ†é…ç»“æœæ•°ç»„
            
            # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
            for i in range(0, total_batches, max_concurrent_batches):
                # è·å–å½“å‰å¹¶å‘æ‰¹æ¬¡
                current_batches = batches[i:i + max_concurrent_batches]
                current_indices = list(range(i, min(i + max_concurrent_batches, total_batches)))
                
                # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡ç»„
                tasks = [
                    process_batch(idx, batch) 
                    for idx, batch in zip(current_indices, current_batches)
                ]
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡ç»„å®Œæˆ
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        # å¼‚å¸¸å¤„ç†
                        yield f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {str(result)}\n"
                        continue
                    
                    batch_index, batch_result = result
                    all_results[batch_index] = batch_result

            # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
            yield "# ğŸ“‹ å¤§å‹å˜æ›´å®¡æŸ¥æŠ¥å‘Š\n"
            yield f"æ€»è®¡ {total_files} ä¸ªæ–‡ä»¶ï¼Œåˆ† {total_batches} ä¸ªæ‰¹æ¬¡å¹¶è¡Œå¤„ç†\n\n"
            
            # æŒ‰é¡ºåºè¾“å‡ºå„æ‰¹æ¬¡ç»“æœ
            for i, result in enumerate(all_results, 1):
                if result is not None:
                    yield f"## æ‰¹æ¬¡ {i} å®¡æŸ¥ç»“æœ\n"
                    yield f"{result}\n"
                    yield "\n---\n\n"
                else:
                    yield f"## æ‰¹æ¬¡ {i} å¤„ç†å¤±è´¥\n\n"

        except Exception as e:
            yield f"å¤§å‹å˜æ›´å®¡æ ¸å¤±è´¥: {str(e)}"
    def _get_text(self, content: Union[TextContent, ImageContent, EmbeddedResource]) -> Optional[str]:
        """
        Extract text content from a content object if available.

        Args:
            content: A content object (TextContent, ImageContent, or EmbeddedResource)

        Returns:
            The text content as a string or None if not a text content
        """
        # print(f"get_text: {content}")
        if isinstance(content, TextContent):
            return self._get_text(content.text)

        if isinstance(content, TextResourceContents):
            return self._get_text(content.text)

        if isinstance(content, EmbeddedResource):
            if isinstance(content.resource, TextResourceContents):
                return self._get_text(content.resource.text)

        if isinstance(content, str):
            return content    

        return None
